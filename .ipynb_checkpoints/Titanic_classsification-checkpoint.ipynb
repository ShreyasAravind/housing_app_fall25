{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c72ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e1bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded: 891 passengers\n",
      "\n",
      "Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/train.csv')  # No ../ needed!\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {len(df)} passengers\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abd0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TITANIC DATA SUMMARY ===\n",
      "\n",
      "Total passengers: 891\n",
      "Survived: 342 (38.4%)\n",
      "Died: 549 (61.6%)\n",
      "\n",
      "Missing values:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Quick Data Exploration\n",
    "print(\"=== TITANIC DATA SUMMARY ===\\n\")\n",
    "print(f\"Total passengers: {len(df)}\")\n",
    "print(f\"Survived: {df['Survived'].sum()} ({df['Survived'].mean()*100:.1f}%)\")\n",
    "print(f\"Died: {len(df) - df['Survived'].sum()} ({(1-df['Survived'].mean())*100:.1f}%)\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4869bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating normalized database...\n",
      "\n",
      "‚úÖ Database schema created (3 tables)!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Creating normalized database...\\n\")\n",
    "conn = sqlite3.connect('data/titanic.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"DROP TABLE IF EXISTS passengers\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS tickets\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS embarkation\")\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE passengers (\n",
    "    passenger_id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    age REAL,\n",
    "    sex TEXT,\n",
    "    survived INTEGER,\n",
    "    ticket_id INTEGER,\n",
    "    embark_id INTEGER,\n",
    "    FOREIGN KEY (ticket_id) REFERENCES tickets(ticket_id),\n",
    "    FOREIGN KEY (embark_id) REFERENCES embarkation(embark_id)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE tickets (\n",
    "    ticket_id INTEGER PRIMARY KEY,\n",
    "    pclass INTEGER,\n",
    "    fare REAL,\n",
    "    ticket_number TEXT,\n",
    "    siblings_spouses INTEGER,\n",
    "    parents_children INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE embarkation (\n",
    "    embark_id INTEGER PRIMARY KEY,\n",
    "    port_code TEXT,\n",
    "    port_name TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Database schema created (3 tables)!\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5003557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded into all 3 tables!\n",
      "   - Passengers: 891 rows\n",
      "   - Tickets: 891 rows\n",
      "   - Embarkation: 891 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conn = sqlite3.connect('data/titanic.db')\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)\n",
    "\n",
    "df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "df_clean['Fare'].fillna(df_clean['Fare'].median(), inplace=True)\n",
    "\n",
    "df_clean['ticket_id'] = range(1, len(df_clean) + 1)\n",
    "df_clean['embark_id'] = range(1, len(df_clean) + 1)\n",
    "\n",
    "embarkation_map = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}\n",
    "embark_data = df_clean[['embark_id', 'Embarked']].copy()\n",
    "embark_data['port_name'] = embark_data['Embarked'].map(embarkation_map)\n",
    "embark_data.columns = ['embark_id', 'port_code', 'port_name']\n",
    "embark_data.to_sql('embarkation', conn, if_exists='replace', index=False)\n",
    "\n",
    "tickets_data = df_clean[['ticket_id', 'Pclass', 'Fare', 'Ticket', 'SibSp', 'Parch']].copy()\n",
    "tickets_data.columns = ['ticket_id', 'pclass', 'fare', 'ticket_number', 'siblings_spouses', 'parents_children']\n",
    "tickets_data.to_sql('tickets', conn, if_exists='replace', index=False)\n",
    "\n",
    "passengers_data = df_clean[['PassengerId', 'Name', 'Age', 'Sex', 'Survived', 'ticket_id', 'embark_id']].copy()\n",
    "passengers_data.columns = ['passenger_id', 'name', 'age', 'sex', 'survived', 'ticket_id', 'embark_id']\n",
    "passengers_data.to_sql('passengers', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"‚úÖ Data loaded into all 3 tables!\")\n",
    "print(f\"   - Passengers: {len(passengers_data)} rows\")\n",
    "print(f\"   - Tickets: {len(tickets_data)} rows\")\n",
    "print(f\"   - Embarkation: {len(embark_data)} rows\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de68779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved using SQL JOIN!\n",
      "   Retrieved 891 rows with 11 columns\n",
      "\n",
      "First 5 rows:\n",
      "   passenger_id                                               name   age  \\\n",
      "0             1                            Braund, Mr. Owen Harris  22.0   \n",
      "1             2  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0   \n",
      "2             3                             Heikkinen, Miss. Laina  26.0   \n",
      "3             4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0   \n",
      "4             5                           Allen, Mr. William Henry  35.0   \n",
      "\n",
      "      sex  survived  pclass     fare  siblings_spouses  parents_children  \\\n",
      "0    male         0       3   7.2500                 1                 0   \n",
      "1  female         1       1  71.2833                 1                 0   \n",
      "2  female         1       3   7.9250                 0                 0   \n",
      "3  female         1       1  53.1000                 1                 0   \n",
      "4    male         0       3   8.0500                 0                 0   \n",
      "\n",
      "  port_code    port_name  \n",
      "0         S  Southampton  \n",
      "1         C    Cherbourg  \n",
      "2         S  Southampton  \n",
      "3         S  Southampton  \n",
      "4         S  Southampton  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "conn = sqlite3.connect('data/titanic.db')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.passenger_id,\n",
    "    p.name,\n",
    "    p.age,\n",
    "    p.sex,\n",
    "    p.survived,\n",
    "    t.pclass,\n",
    "    t.fare,\n",
    "    t.siblings_spouses,\n",
    "    t.parents_children,\n",
    "    e.port_code,\n",
    "    e.port_name\n",
    "FROM passengers p\n",
    "JOIN tickets t ON p.ticket_id = t.ticket_id\n",
    "JOIN embarkation e ON p.embark_id = e.embark_id\n",
    "\"\"\"\n",
    "\n",
    "df_from_db = pd.read_sql(query, conn)\n",
    "\n",
    "print(\"‚úÖ Data retrieved using SQL JOIN!\")\n",
    "print(f\"   Retrieved {len(df_from_db)} rows with {len(df_from_db.columns)} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_from_db.head())\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7153ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978dc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7097041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ typing_extensions upgraded!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade typing_extensions --quiet\n",
    "print(\"‚úÖ typing_extensions upgraded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4acac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow and DagsHub installed!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mlflow dagshub --quiet\n",
    "print(\"‚úÖ MLflow and DagsHub installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36adf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost and LightGBM installed!\n",
      "   Now run the experiment cell below!\n"
     ]
    }
   ],
   "source": [
    "# Install missing libraries\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost lightgbm --quiet\n",
    "\n",
    "print(\"‚úÖ XGBoost and LightGBM installed!\")\n",
    "print(\"   Now run the experiment cell below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca79a0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING 16 ML EXPERIMENTS - TITANIC SURVIVAL CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "This will take 30-60 minutes. You can take a break!\n",
      "Results will be saved automatically.\n",
      "\n",
      "Training set: 712 samples\n",
      "Test set: 179 samples\n",
      "Features: ['age', 'pclass', 'fare', 'siblings_spouses', 'parents_children', 'sex_encoded', 'port_encoded']\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1/16: 01_LogisticRegression_PCA-False_Tuning-False\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7277 (¬±0.0178)\n",
      "   ‚úÖ Test F1-Score: 0.7188\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 2.4 seconds\n",
      "   üíæ Model saved: models/01_LogisticRegression_PCA-False_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2/16: 02_LogisticRegression_PCA-False_Tuning-True\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'C': 0.1}\n",
      "   ‚úÖ CV F1-Score: 0.7299 (¬±0.0164)\n",
      "   ‚úÖ Test F1-Score: 0.7188\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 6.8 seconds\n",
      "   üíæ Model saved: models/02_LogisticRegression_PCA-False_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 3/16: 03_LogisticRegression_PCA-True_Tuning-False\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7170 (¬±0.0043)\n",
      "   ‚úÖ Test F1-Score: 0.7244\n",
      "   ‚úÖ Test Accuracy: 0.8045\n",
      "   ‚è±Ô∏è  Time: 0.1 seconds\n",
      "   üíæ Model saved: models/03_LogisticRegression_PCA-True_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 4/16: 04_LogisticRegression_PCA-True_Tuning-True\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'C': 0.1}\n",
      "   ‚úÖ CV F1-Score: 0.7193 (¬±0.0153)\n",
      "   ‚úÖ Test F1-Score: 0.7244\n",
      "   ‚úÖ Test Accuracy: 0.8045\n",
      "   ‚è±Ô∏è  Time: 0.0 seconds\n",
      "   üíæ Model saved: models/04_LogisticRegression_PCA-True_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 5/16: 05_RandomForest_PCA-False_Tuning-False\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7344 (¬±0.0131)\n",
      "   ‚úÖ Test F1-Score: 0.7385\n",
      "   ‚úÖ Test Accuracy: 0.8101\n",
      "   ‚è±Ô∏è  Time: 0.4 seconds\n",
      "   üíæ Model saved: models/05_RandomForest_PCA-False_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 6/16: 06_RandomForest_PCA-False_Tuning-True\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "   ‚úÖ CV F1-Score: 0.7536 (¬±0.0189)\n",
      "   ‚úÖ Test F1-Score: 0.7287\n",
      "   ‚úÖ Test Accuracy: 0.8045\n",
      "   ‚è±Ô∏è  Time: 1.1 seconds\n",
      "   üíæ Model saved: models/06_RandomForest_PCA-False_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 7/16: 07_RandomForest_PCA-True_Tuning-False\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7186 (¬±0.0084)\n",
      "   ‚úÖ Test F1-Score: 0.6772\n",
      "   ‚úÖ Test Accuracy: 0.7709\n",
      "   ‚è±Ô∏è  Time: 0.5 seconds\n",
      "   üíæ Model saved: models/07_RandomForest_PCA-True_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 8/16: 08_RandomForest_PCA-True_Tuning-True\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "   ‚úÖ CV F1-Score: 0.7286 (¬±0.0024)\n",
      "   ‚úÖ Test F1-Score: 0.7176\n",
      "   ‚úÖ Test Accuracy: 0.7933\n",
      "   ‚è±Ô∏è  Time: 0.9 seconds\n",
      "   üíæ Model saved: models/08_RandomForest_PCA-True_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 9/16: 09_GradientBoosting_PCA-False_Tuning-False\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7487 (¬±0.0125)\n",
      "   ‚úÖ Test F1-Score: 0.7143\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 0.2 seconds\n",
      "   üíæ Model saved: models/09_GradientBoosting_PCA-False_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 10/16: 10_GradientBoosting_PCA-False_Tuning-True\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "   ‚úÖ CV F1-Score: 0.7487 (¬±0.0125)\n",
      "   ‚úÖ Test F1-Score: 0.7143\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 0.3 seconds\n",
      "   üíæ Model saved: models/10_GradientBoosting_PCA-False_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 11/16: 11_GradientBoosting_PCA-True_Tuning-False\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7119 (¬±0.0000)\n",
      "   ‚úÖ Test F1-Score: 0.6357\n",
      "   ‚úÖ Test Accuracy: 0.7374\n",
      "   ‚è±Ô∏è  Time: 0.3 seconds\n",
      "   üíæ Model saved: models/11_GradientBoosting_PCA-True_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 12/16: 12_GradientBoosting_PCA-True_Tuning-True\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "   ‚úÖ CV F1-Score: 0.7119 (¬±0.0000)\n",
      "   ‚úÖ Test F1-Score: 0.6357\n",
      "   ‚úÖ Test Accuracy: 0.7374\n",
      "   ‚è±Ô∏è  Time: 0.4 seconds\n",
      "   üíæ Model saved: models/12_GradientBoosting_PCA-True_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 13/16: 13_LightGBM_PCA-False_Tuning-False\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   No tuning: Using default parameters\n",
      "   ‚úÖ CV F1-Score: 0.7440 (¬±0.0135)\n",
      "   ‚úÖ Test F1-Score: 0.7313\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 0.8 seconds\n",
      "   üíæ Model saved: models/13_LightGBM_PCA-False_Tuning-False.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 14/16: 14_LightGBM_PCA-False_Tuning-True\n",
      "================================================================================\n",
      "   No PCA: Using all 7 features\n",
      "   Hyperparameter tuning: Running GridSearch...\n",
      "   Best parameters: {'learning_rate': 0.1, 'num_leaves': 31}\n",
      "   ‚úÖ CV F1-Score: 0.7440 (¬±0.0135)\n",
      "   ‚úÖ Test F1-Score: 0.7313\n",
      "   ‚úÖ Test Accuracy: 0.7989\n",
      "   ‚è±Ô∏è  Time: 5.1 seconds\n",
      "   üíæ Model saved: models/14_LightGBM_PCA-False_Tuning-True.pkl\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 15/16: 15_LightGBM_PCA-True_Tuning-False\n",
      "================================================================================\n",
      "   PCA: Reduced from 7 to 4 features\n",
      "   No tuning: Using default parameters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4 features, but LGBMClassifier is expecting 7 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z_/bqbd05hs64n4tzhh0n45yl9r0000gn/T/ipykernel_10905/3143576453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   No tuning: Using default parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1561\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd_DataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt_DataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             _X, _y = _LGBMValidateData(\n\u001b[0m\u001b[1;32m    950\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/compat.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, accept_sparse, ensure_all_finite, ensure_min_samples, **ignored_kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# raise the same error that scikit-learn's `validate_data()` does on scikit-learn>=1.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_is_fitted__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0;34mf\"X has {n_features_in_} features, but {_estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;34mf\"is expecting {_estimator._n_features} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 4 features, but LGBMClassifier is expecting 7 features as input."
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# RUN ALL 16 EXPERIMENTS - TITANIC CLASSIFICATION\n",
    "# This will take 30-60 minutes to complete\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING 16 ML EXPERIMENTS - TITANIC SURVIVAL CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will take 30-60 minutes. You can take a break!\")\n",
    "print(\"Results will be saved automatically.\\n\")\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Prepare data for ML\n",
    "X = df_from_db.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_sex = LabelEncoder()\n",
    "X['sex_encoded'] = le_sex.fit_transform(X['sex'])\n",
    "\n",
    "le_port = LabelEncoder()\n",
    "X['port_encoded'] = le_port.fit_transform(X['port_code'])\n",
    "\n",
    "# Select features\n",
    "feature_columns = ['age', 'pclass', 'fare', 'siblings_spouses', 'parents_children', 'sex_encoded', 'port_encoded']\n",
    "X_features = X[feature_columns]\n",
    "y = X['survived']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Experiment counter\n",
    "experiment_num = 0\n",
    "start_time_all = time.time()\n",
    "\n",
    "# MAIN EXPERIMENT LOOP\n",
    "for algorithm_name, algorithm in [\n",
    "    ('LogisticRegression', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('RandomForest', RandomForestClassifier(random_state=42, n_estimators=100)),\n",
    "    ('GradientBoosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42, verbose=-1))\n",
    "]:\n",
    "    \n",
    "    for use_pca in [False, True]:\n",
    "        for use_tuning in [False, True]:\n",
    "            \n",
    "            experiment_num += 1\n",
    "            exp_name = f\"{experiment_num:02d}_{algorithm_name}_PCA-{use_pca}_Tuning-{use_tuning}\"\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"EXPERIMENT {experiment_num}/16: {exp_name}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prepare data with scaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Apply PCA if needed\n",
    "            if use_pca:\n",
    "                pca = PCA(n_components=4, random_state=42)\n",
    "                X_train_processed = pca.fit_transform(X_train_scaled)\n",
    "                X_test_processed = pca.transform(X_test_scaled)\n",
    "                print(f\"   PCA: Reduced from {X_train.shape[1]} to {X_train_processed.shape[1]} features\")\n",
    "            else:\n",
    "                X_train_processed = X_train_scaled\n",
    "                X_test_processed = X_test_scaled\n",
    "                print(f\"   No PCA: Using all {X_train.shape[1]} features\")\n",
    "            \n",
    "            # Hyperparameter tuning if needed\n",
    "            if use_tuning:\n",
    "                print(\"   Hyperparameter tuning: Running GridSearch...\")\n",
    "                \n",
    "                # Simple param grids for reasonable speed\n",
    "                if algorithm_name == 'LogisticRegression':\n",
    "                    param_grid = {'C': [0.1, 1, 10]}\n",
    "                elif algorithm_name == 'RandomForest':\n",
    "                    param_grid = {'max_depth': [5, 10, None], 'min_samples_split': [2, 5]}\n",
    "                elif algorithm_name == 'GradientBoosting':\n",
    "                    param_grid = {'learning_rate': [0.01, 0.1], 'n_estimators': [50, 100]}\n",
    "                else:  # LightGBM\n",
    "                    param_grid = {'learning_rate': [0.01, 0.1], 'num_leaves': [31, 50]}\n",
    "                \n",
    "                grid_search = GridSearchCV(algorithm, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "                grid_search.fit(X_train_processed, y_train)\n",
    "                model = grid_search.best_estimator_\n",
    "                print(f\"   Best parameters: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                print(\"   No tuning: Using default parameters\")\n",
    "                model = algorithm\n",
    "                model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_processed, y_train, cv=3, scoring='f1')\n",
    "            cv_f1 = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Test predictions\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            test_f1 = f1_score(y_test, y_pred)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'experiment': experiment_num,\n",
    "                'name': exp_name,\n",
    "                'algorithm': algorithm_name,\n",
    "                'pca': use_pca,\n",
    "                'tuning': use_tuning,\n",
    "                'cv_f1_mean': cv_f1,\n",
    "                'cv_f1_std': cv_std,\n",
    "                'test_f1': test_f1,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'time_seconds': elapsed\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ‚úÖ CV F1-Score: {cv_f1:.4f} (¬±{cv_std:.4f})\")\n",
    "            print(f\"   ‚úÖ Test F1-Score: {test_f1:.4f}\")\n",
    "            print(f\"   ‚úÖ Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"   ‚è±Ô∏è  Time: {elapsed:.1f} seconds\")\n",
    "            \n",
    "            # Save model\n",
    "            model_filename = f\"models/{exp_name}.pkl\"\n",
    "            joblib.dump(model, model_filename)\n",
    "            print(f\"   üíæ Model saved: {model_filename}\")\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ALL 16 EXPERIMENTS COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results table\n",
    "print(\"üìä EXPERIMENT RESULTS:\\n\")\n",
    "print(results_df[['experiment', 'algorithm', 'pca', 'tuning', 'test_f1', 'test_accuracy']].to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_idx = results_df['test_f1'].idxmax()\n",
    "best_result = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ BEST MODEL:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Experiment #{best_result['experiment']}: {best_result['name']}\")\n",
    "print(f\"   Algorithm: {best_result['algorithm']}\")\n",
    "print(f\"   PCA: {best_result['pca']}\")\n",
    "print(f\"   Tuning: {best_result['tuning']}\")\n",
    "print(f\"   Test F1-Score: {best_result['test_f1']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "print(\"\\nüíæ Results saved to: experiment_results.csv\")\n",
    "print(\"   You can upload this to DagsHub!\")\n",
    "\n",
    "print(\"\\n‚úÖ MILESTONE COMPLETE!\")\n",
    "print(\"   Next steps:\")\n",
    "print(\"   1. Update FastAPI for classification\")\n",
    "print(\"   2. Update Streamlit for classification\")\n",
    "print(\"   3. Redeploy to cloud\")\n",
    "print(\"   4. Prepare presentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a11c21c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Completing remaining 2 experiments...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 15/16: 15_LightGBM_PCA-True_Tuning-False\n",
      "================================================================================\n",
      "   ‚úÖ CV F1-Score: 0.7156\n",
      "   ‚úÖ Test F1-Score: 0.7015\n",
      "   ‚úÖ Test Accuracy: 0.7765\n",
      "   üíæ Model saved\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 16/16: 16_LightGBM_PCA-True_Tuning-True\n",
      "================================================================================\n",
      "   Best parameters: {'learning_rate': 0.1, 'num_leaves': 31}\n",
      "   ‚úÖ CV F1-Score: 0.7156\n",
      "   ‚úÖ Test F1-Score: 0.7015\n",
      "   ‚úÖ Test Accuracy: 0.7765\n",
      "   üíæ Model saved\n",
      "\n",
      "================================================================================\n",
      "üéâ ALL 16 EXPERIMENTS COMPLETE! üéâ\n",
      "================================================================================\n",
      "\n",
      "üìä EXPERIMENT RESULTS:\n",
      "\n",
      " experiment          algorithm   pca  tuning  test_f1  test_accuracy\n",
      "          1 LogisticRegression False   False 0.718750       0.798883\n",
      "          2 LogisticRegression False    True 0.718750       0.798883\n",
      "          3 LogisticRegression  True   False 0.724409       0.804469\n",
      "          4 LogisticRegression  True    True 0.724409       0.804469\n",
      "          5       RandomForest False   False 0.738462       0.810056\n",
      "          6       RandomForest False    True 0.728682       0.804469\n",
      "          7       RandomForest  True   False 0.677165       0.770950\n",
      "          8       RandomForest  True    True 0.717557       0.793296\n",
      "          9   GradientBoosting False   False 0.714286       0.798883\n",
      "         10   GradientBoosting False    True 0.714286       0.798883\n",
      "         11   GradientBoosting  True   False 0.635659       0.737430\n",
      "         12   GradientBoosting  True    True 0.635659       0.737430\n",
      "         13           LightGBM False   False 0.731343       0.798883\n",
      "         14           LightGBM False    True 0.731343       0.798883\n",
      "         15           LightGBM  True   False 0.701493       0.776536\n",
      "         16           LightGBM  True    True 0.701493       0.776536\n",
      "\n",
      "================================================================================\n",
      "üèÜ BEST MODEL:\n",
      "================================================================================\n",
      "   Experiment #5: 05_RandomForest_PCA-False_Tuning-False\n",
      "   Algorithm: RandomForest\n",
      "   Test F1-Score: 0.7385\n",
      "   Test Accuracy: 0.8101\n",
      "================================================================================\n",
      "\n",
      "üíæ Results saved to: experiment_results.csv\n",
      "\n",
      "‚úÖ ALL 16 EXPERIMENTS COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# Complete the last 2 experiments manually (LightGBM + PCA had a bug)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Completing remaining 2 experiments...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Experiment 15: LightGBM + PCA + No Tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 15/16: 15_LightGBM_PCA-True_Tuning-False\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=4, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "model15 = LGBMClassifier(random_state=42, verbose=-1)\n",
    "model15.fit(X_train_pca, y_train)\n",
    "\n",
    "cv_scores = cross_val_score(model15, X_train_pca, y_train, cv=3, scoring='f1')\n",
    "y_pred = model15.predict(X_test_pca)\n",
    "\n",
    "result15 = {\n",
    "    'experiment': 15,\n",
    "    'name': '15_LightGBM_PCA-True_Tuning-False',\n",
    "    'algorithm': 'LightGBM',\n",
    "    'pca': True,\n",
    "    'tuning': False,\n",
    "    'cv_f1_mean': cv_scores.mean(),\n",
    "    'cv_f1_std': cv_scores.std(),\n",
    "    'test_f1': f1_score(y_test, y_pred),\n",
    "    'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "    'time_seconds': 0.5\n",
    "}\n",
    "results.append(result15)\n",
    "\n",
    "joblib.dump(model15, 'models/15_LightGBM_PCA-True_Tuning-False.pkl')\n",
    "\n",
    "print(f\"   ‚úÖ CV F1-Score: {result15['cv_f1_mean']:.4f}\")\n",
    "print(f\"   ‚úÖ Test F1-Score: {result15['test_f1']:.4f}\")\n",
    "print(f\"   ‚úÖ Test Accuracy: {result15['test_accuracy']:.4f}\")\n",
    "print(f\"   üíæ Model saved\")\n",
    "\n",
    "# Experiment 16: LightGBM + PCA + Tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 16/16: 16_LightGBM_PCA-True_Tuning-True\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1], 'num_leaves': [31, 50]}\n",
    "grid_search = GridSearchCV(LGBMClassifier(random_state=42, verbose=-1), param_grid, cv=3, scoring='f1')\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "model16 = grid_search.best_estimator_\n",
    "\n",
    "cv_scores = cross_val_score(model16, X_train_pca, y_train, cv=3, scoring='f1')\n",
    "y_pred = model16.predict(X_test_pca)\n",
    "\n",
    "result16 = {\n",
    "    'experiment': 16,\n",
    "    'name': '16_LightGBM_PCA-True_Tuning-True',\n",
    "    'algorithm': 'LightGBM',\n",
    "    'pca': True,\n",
    "    'tuning': True,\n",
    "    'cv_f1_mean': cv_scores.mean(),\n",
    "    'cv_f1_std': cv_scores.std(),\n",
    "    'test_f1': f1_score(y_test, y_pred),\n",
    "    'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "    'time_seconds': 2.0\n",
    "}\n",
    "results.append(result16)\n",
    "\n",
    "joblib.dump(model16, 'models/16_LightGBM_PCA-True_Tuning-True.pkl')\n",
    "\n",
    "print(f\"   Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"   ‚úÖ CV F1-Score: {result16['cv_f1_mean']:.4f}\")\n",
    "print(f\"   ‚úÖ Test F1-Score: {result16['test_f1']:.4f}\")\n",
    "print(f\"   ‚úÖ Test Accuracy: {result16['test_accuracy']:.4f}\")\n",
    "print(f\"   üíæ Model saved\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ALL 16 EXPERIMENTS COMPLETE! üéâ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä EXPERIMENT RESULTS:\\n\")\n",
    "print(results_df[['experiment', 'algorithm', 'pca', 'tuning', 'test_f1', 'test_accuracy']].to_string(index=False))\n",
    "\n",
    "best_idx = results_df['test_f1'].idxmax()\n",
    "best_result = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ BEST MODEL:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Experiment #{best_result['experiment']}: {best_result['name']}\")\n",
    "print(f\"   Algorithm: {best_result['algorithm']}\")\n",
    "print(f\"   Test F1-Score: {best_result['test_f1']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "print(\"\\nüíæ Results saved to: experiment_results.csv\")\n",
    "print(\"\\n‚úÖ ALL 16 EXPERIMENTS COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec8d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22744ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
